\documentclass[12pt, oneside]{article}
\usepackage[letterpaper]{geometry}
\usepackage{Logemann}
\usepackage{SetTheory}
\usepackage{Sum}
\usepackage{Product}
\usepackage{listings}
\usepackage{jlcode}

\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
AERE 504 Intelligent Air Systems \\
Project 2
}}

\section{Description of Methods}
  \subsection{Part 1 - Discrete Grid World}
    For this problem I formed the transition and reward function from the data
    using the maximum liklihood approach.
    After forming these function I used the Gauss-Seidel Value iteration to
    compute the maximal Utility of each state and then extracted the policy
    from $U$.
    This algorithm took approximately 2 seconds to run on the given data.

  \subsection{Part 2 - Continuous Grid World}
    For this problem I followed the approach of Xuxi Yang in his research.
    I implemented a tree search to compute the proper policy on the fly during
    the simulation.
    The algorithm searched out to all possible sequences of 6 actions.
    At each state, $s$ the reward is computed as
    \[
      r_s = 1 - d_s/d_{max}
    \]
    where $d_s$ is the distance to the goal position at this state and $d_{max}$ is
    the maximum distance possible that is
    \[
      d_{max} = \sqrt{500^2 + 500^2} = 500\sqrt{2}
    \]
    If the state is invalid, that is the state violates the boundaries of the region,
    then the reward is $-1$.
    There is also a reward associated with each node in the tree.
    Each node in the tree has a state associated with it, that state is generated by
    updating the state of the parent node assuming a given action was taken.
    The state of the root node is the current state.
    The reward for a leaf node is just the reward for the state associated with
    that node.
    If a state is not a leaf state the the reward for the node is an average
    of the reward at that state and it children's rewards.
    The reward is computed as
    \[
      r_n = \frac{1}{2}r_s + \frac{1}{2} \frac{r_l + r_r + r_0}{3}
    \]
    where $r_s$ is the reward of the state of the node and $r_l, r_r, r_0$ are
    the rewards of the nodes children.

    The algorithm computes the reward of all three possible actions and then
    chooses the action with the highest reward.
    Since this policy is computed as the simulation runs, it takes no longer then
    the original simulation took to run and no precomputation is necessary.
    However if the tree depth is increased some slow down of the simulation is
    present. The depth of the tree can go to 7 or 8 actions before the simulation 
    is noticeably slowed.

    The algorithm almost always reaches the goal state.
    The only cases where it does not is when the goal state is very close to the
    initial position of the plane and the plane cannot turn quickly enough to
    reach the goal.
    In this case the plan runs into the wall close to the goal state.

\section{Appendix}
  \subsection{Part 1 - Discrete Grid World}
    This first Julia file implements, the Value Iteration, QLearning, and SARSA
    algorithms for finding a policy.
    \lstinputlisting[breaklines=true]{ReinforcementLearning.jl}
    This script uses the previous function to actually compute the policy
    for the given data.
    \lstinputlisting[breaklines=true]{Project2.jl}

  \subsection{Part 2 - Continuous Grid World}
    The following python code implements my tree search policy
    for part 2 of the project.
    In order to use this code,
    "from part2policy import *" needs to be added to the import statements of
    the simulator file.
    \lstinputlisting[language=Python]{part2policy.py}

\end{document}

